---
logs:
  json: false
metrics:
  # Prometheus metrics available on `/metrics` endpoint
  # Readiness probe available on `/ready`
  # Liveness probe available on `/health`
  endpoint: 127.0.0.1:8001
source:
  # Tokio runtime for RPC requests and Geyser stream
  tokio:
    worker_threads: null # Number of threads in Tokio runtime, by default all available cores
    affinity: null # Pin threads to specific CPUs, optional (taskset syntax)
  # RPC endpoint, used for edge-cases, stream reconnect, catch-up on restart, and backfilling
  # HttpGet endpoint can be used for catch-up on restart and backfilling (can be skipped)
  http:
    rpc: http://127.0.0.1:8899/
    httpget: null
    timeout: 30s
    concurrency: 10 # Max number of requests in progress at one moment
  # Geyser stream
  stream:
    source: richat # richat (richat-geyser-plugin) or dragons_mouth (yellowstone grpc)
    reconnect: # Reconnect options, if `null` then reconnect is disabled
      backoff_init: 100ms
      backoff_max: 1s # Max delay between reconnect attempts
    endpoint: http://127.0.0.1:10000
    ca_certificate: null
    connect_timeout: null
    buffer_size: null
    http2_adaptive_window: null
    http2_keep_alive_interval: null
    initial_connection_window_size: null
    initial_stream_window_size: null
    keep_alive_timeout: null
    keep_alive_while_idle: false
    tcp_keepalive: 15s
    tcp_nodelay: true
    timeout: null
    max_decoding_message_size: 4_194_304 # 4MiB, better set to 16MiB (because account messages can be more than 10MiB)
    compression: # Optional compression, not recommended due to high traffic volume
      accept: [] # valid: gzip, zstd
      send: [] # valid: gzip, zstd
    x_token: null
storage:
  # Backfill db to specified slot, if missed no backfilling at all
  # If you want to reach max blocks then set to 0 would be easiest solution
  # backfilling:
  #   sync_to: 338_669_763
  blocks:
    max: 1_000 # Max number of stored slots, once limit is reached old blocks will be pruned
    http_getblock_max_retries: 10 # Max retries when trying to fetch blocks from RPC
    http_getblock_backoff_init: 100ms # Initial exponentional retry delay
    # Files to store blocks in protobuf, can be on different disks
    # Used in round-robin schedule. Once all space is used old blocks will be pruned
    files:
      - id: 0
        path: ./db/alpamayo/storage0
        size: 8gb
        new_blocks: true
  rocksdb:
    path: ./db/alpamayo/rocksdb # Rocksdb location, don't forget to increase `max_open_files`
    index_slot_compression: none # Slot info (transactions and touched addresses) compression: none, snappy, zlib, bz2, lz4, lz4hc, zstd
    index_sfa_compression: none # Address signatures per slot compression: none, snappy, zlib, bz2, lz4, lz4hc, zstd
    read_workers: null # Number of threads used to read data from Rocksdb, by default number of CPUs on the system
  write:
    affinity: null # Optional affinity of write thread, used to build blocks and write them to storage files
  read:
    threads: 1 # Number of single-threaded tokio runtimes used to read data from storage files
    affinity: null # Optional affinity of read tokio runtimes
    # Max number of async requests handled by every read thread
    # async requests: getBlock, getInflationReward, getSignaturesForAddress, getSignatureStatuses, getTransaction
    thread_max_async_requests: 1024
    # Max number of read requests from files storage, includes: getBlock, getTransaction
    thread_max_files_requests: 32
rpc:
  endpoint: 127.0.0.1:9000 # RPC endpoint, implement same JSON-RPC API as Agave
  # RPC Tokio runtime, used to parse requests, serialize responses
  tokio:
    worker_threads: null # Number of threads in Tokio runtime, by default all available cores
    affinity: null # Pin threads to specified CPUs, optional
  body_limit: 50KiB # JSON-RPC body limit
  extra_headers: # Extra headers added to response
    # access-control-allow-origin: "*"
    # access-control-max-age: 86400
    # access-control-allow-methods: OPTIONS, POST
  request_timeout: 60s
  # httpget is plain HTTP interface with protobuf in response, allow to server blocks up to 15 times faster
  # `/block/${slot}` — block request
  # `/tx/${signature}` — transaction request
  calls_httpget:
    - getBlock
    - getTransaction
  # Supported methods in JSON-RPC
  calls_jsonrpc:
    - getBlock
    - getBlockHeight
    - getBlocks
    - getBlocksWithLimit
    - getBlockTime
    - getClusterNodes
    - getFirstAvailableBlock
    - getInflationReward
    - getLatestBlockhash
    - getLeaderSchedule
    - getRecentPrioritizationFees
    - getSignaturesForAddress
    - getSignatureStatuses
    - getSlot
    - getTransaction
    - getVersion
    - isBlockhashValid
  gsfa_limit: 1_000 # Max limit in `getSignaturesForAddress`, default value in Agave is 1000
  gss_transaction_history: true # Allow to handle `getSignatureStatus` from storage, not only from latest 300 slots
  grpf_percentile: true # Allow to use `percentile` option in `getRecentPrioritizationFees`
  gcn_cache_ttl: 1s # TTL for cached `getClusterNodes`
  request_channel_capacity: 4_096 # Queue size of requests to read threads (be aware, one JSON-RPC request can contain multiple requests)
  # fallback for httpget
  # upstream_httpget:
  #   - name: main
  #     calls:
  #       - getBlock
  #       - getTransaction
  #     endpoint: http://127.0.0.1:8899
  #     user_agent: alpamayo/v0.1.0
  #     version: HTTP/1.1
  #     timeout: 30s
  # fallback for JSON-RPC methods
  # upstream_jsonrpc:
  #   - name: main
  #     calls:
  #       - getBlock
  #       - getBlockHeight
  #       - getBlocks
  #       - getBlocksWithLimit
  #       - getBlockTime
  #       - getClusterNodes
  #       - getFirstAvailableBlock
  #       - getInflationReward
  #       - getLatestBlockhash
  #       - getLeaderSchedule
  #       - getRecentPrioritizationFees
  #       - getSignaturesForAddress
  #       - getSignatureStatuses
  #       - getSlot
  #       - getTransaction
  #       - getVersion
  #       - isBlockhashValid
  #     endpoint: http://127.0.0.1:8899
  #     user_agent: alpamayo/v0.1.0
  #     version: HTTP/1.1
  #     timeout: 30s
  # getBlock and getTransactions requires a lot of CPU time to decode protobuf, encode to requested format, serialize data
  # getBlock can requires more than 100ms to do all work, it would be good to have these threads separated from everything rest
  # for example, you can pin:
  #   - source runtime + write thread + read threads to one pool of CPUs
  #   - rpc runtime to another pool of CPUs
  #   - workers to some CPUs from rpc runtime
  workers:
    threads: null # Number of workers, by default number of CPUs
    affinity: null # Optional threads affinity
    channel_size: 4_096 # Queue size to worker threads
